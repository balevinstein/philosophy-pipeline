version: 4
data:
  attachedData:
    trivet:
      testSuites: []
      version: 1
  graphs:
    P3VgCXmQMf_2EWIGrVarF:
      metadata:
        description: ""
        id: P3VgCXmQMf_2EWIGrVarF
        name: Literary Research Query
      nodes:
        '[2KVulxz40ZNa4Es5TFhG7]:graphInput "Graph Input"':
          data:
            dataType: string
            id: question
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Chat" ykeGZg8QrHWyjNIL140F_/prompt
          visualData: 608.2591652677022/587.5424459629821/330/6//
        '[2tJHm6t87O5knZpU4sO6k]:text "Text"':
          data:
            text: What can you do?
          outgoingConnections:
            - output->"Graph Input" 2KVulxz40ZNa4Es5TFhG7/default
          visualData: 101.81532968613725/567.2670840803833/330/7//
        '[ALzZ9S_wyWclWFpmT18_k]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: Answer the question asked by the user
            type: system
            useTypeInput: false
          outgoingConnections:
            - output->"Chat" ykeGZg8QrHWyjNIL140F_/systemPrompt
          visualData: 709.1962593776855/311.37479150268354/280/null//
        '[a6BishDCULH9QUwWB-fp2]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1762.2154239023557/582.3899279506428/330/null//
        '[ykeGZg8QrHWyjNIL140F_]:chat "Chat"':
          data:
            additionalParameters: []
            cache: false
            enableFunctionUse: false
            maxTokens: 1024
            modalitiesIncludeAudio: false
            modalitiesIncludeText: false
            model: gpt-4o-mini
            outputUsage: false
            parallelFunctionCalling: true
            reasoningEffort: medium
            stop: ""
            temperature: 0.5
            top_p: 1
            useAdditionalParametersInput: false
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePredictedOutput: false
            usePresencePenaltyInput: false
            useReasoningEffortInput: false
            useServerTokenCalculation: true
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Graph Output" a6BishDCULH9QUwWB-fp2/value
          visualData: 1320.0854914342224/548.7147465918337/230/8//
  metadata:
    description: ""
    id: rBponj37DhN1Lkq2pRSnU
    title: philosphy-pipeline
  plugins: []
